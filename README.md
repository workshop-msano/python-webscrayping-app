# Python Web Scraping with AWS Lambda

[![Python Web Scraping](https://img.shields.io/badge/Python-Web%20Scraping-blue?style=flat-square)](https://github.com/workshop-msano/python-webscrayping-app)
[![License](https://img.shields.io/github/license/workshop-msano/python-webscrayping-app?style=flat-square)](LICENSE)

This is a Python application that demonstrates web scraping techniques using AWS Lambda and the AWS Toolkit for VSCode. It provides a flexible framework for scraping web pages, parsing data, and leveraging AWS serverless functions for scalable web scraping.

## Table of Contents

- [Installation](#installation)
- [Usage](#usage)
- [Contributing](#contributing)
- [License](#license)

## Installation

1. Clone the repository:

   ```shell
   git clone https://github.com/workshop-msano/python-webscrayping-app.git
   ```

2. Navigate to the project directory:

   ```shell
   cd python-webscrayping-app
   ```

3. Install the required dependencies using pip:
   <br/><sub>Creating a virtual environment is strongly recommended.</sub>


   ```shell
   pip install -r requirements.txt
   ```

## Prerequisites

Install and configure AWS CLI & SAM CLI.

## Usage

1. Customize the scraping logic: Open the `scraper.py` file and modify the code to define the specific web scraping rules based on your requirements.

2. Set up AWS Lambda: Follow the AWS documentation to create an AWS Lambda function and configure the necessary permissions and triggers.

3. Configure AWS credentials: Install the AWS Toolkit for VSCode, and set up your AWS credentials using the AWS Command Line Interface (CLI) or VSCode's integrated AWS credentials management.

4. Build the application to AWS Lambda: `sam build`
   
5. Test the app locally: `sam local invoke`

7. Deploy the application to AWS Lambda: `sam deploy --guided`.
   This will also create a samconfig.toml file that will contain these configurations.
   Next time after you build the app, just run sam deploy to deploy the app.

8. Monitor the execution and results: Check the AWS CloudWatch logs and the output generated by the Lambda function to view the scraped data.

## Contributing

Contributions are welcome! If you have any ideas, suggestions, or bug reports, please open an issue or submit a pull request. Your input is highly appreciated.

To contribute to the project, follow these steps:

1. Fork the repository.

2. Create a new branch:

   ```shell
   git checkout -b my-feature-branch
   ```

3. Make your changes and commit them:

   ```shell
   git commit -m "Add new feature"
   ```

4. Push your changes to the forked repository:

   ```shell
   git push origin my-feature-branch
   ```

5. Open a pull request with a detailed description of your changes.

6. Wait for the project maintainers to review and merge your pull request.

## License

This project is licensed under the MIT License. See the [LICENSE](LICENSE) file for details.

---

Feel free to explore, use, and enhance this web scraping application. If you have any questions or need assistance, please don't hesitate to reach out. Happy web scraping!

### Reference 
[Run Selenium in AWS Lambda for UI testing](https://cloudbytes.dev/snippets/run-selenium-in-aws-lambda-for-ui-testing)
